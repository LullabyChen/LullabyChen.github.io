<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="utf-8">
  
  <title>KERAS学习（二）：电影评级分类-二分类问题 | Cabin</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  
  <meta name="keywords" content="Keras">
  
  
  
  
  <meta name="description" content="123#加载IMDB数据集from keras.datasets import imdb(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) 1Using TensorFlow backend.">
<meta name="keywords" content="Keras">
<meta property="og:type" content="article">
<meta property="og:title" content="KERAS学习（二）：电影评级分类-二分类问题">
<meta property="og:url" content="https://lullabychen.github.io/2019/01/28/Keras2/index.html">
<meta property="og:site_name" content="Cabin">
<meta property="og:description" content="123#加载IMDB数据集from keras.datasets import imdb(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) 1Using TensorFlow backend.">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://lullabychen.github.io/2019/01/28/Keras2/0.png">
<meta property="og:updated_time" content="2019-01-28T12:17:09.855Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="KERAS学习（二）：电影评级分类-二分类问题">
<meta name="twitter:description" content="123#加载IMDB数据集from keras.datasets import imdb(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) 1Using TensorFlow backend.">
<meta name="twitter:image" content="https://lullabychen.github.io/2019/01/28/Keras2/0.png">
  
    <link rel="alternate" href="/atom.xml" title="Cabin" type="application/atom+xml">
  
  <link rel="icon" href="/css/images/favicon.ico">
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link href="https://fonts.googleapis.com/css?family=Open+Sans|Montserrat:700" rel="stylesheet" type="text/css">
  <link href="https://fonts.googleapis.com/css?family=Roboto:400,300,300italic,400italic" rel="stylesheet" type="text/css">
  <link href="//cdn.bootcss.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet">
  <style type="text/css">
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/9749f0/00000000000000000001008f/27/l?subset_id=2&fvd=n5) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/90cf9f/000000000000000000010091/27/l?subset_id=2&fvd=n7) format("woff2");font-weight:500;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/8a5494/000000000000000000013365/27/l?subset_id=2&fvd=n4) format("woff2");font-weight:lighter;font-style:normal;}
    @font-face{font-family:futura-pt;src:url(https://use.typekit.net/af/d337d8/000000000000000000010095/27/l?subset_id=2&fvd=i4) format("woff2");font-weight:400;font-style:italic;}</style>
    
  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Yanone+Kaffeesatz%3A200%2C300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">

  <link rel="stylesheet" id="athemes-headings-fonts-css" href="//fonts.googleapis.com/css?family=Oswald%3A300%2C400%2C700&amp;ver=4.6.1" type="text/css" media="all">
  <link rel="stylesheet" href="/css/style.css">

  <script src="/js/jquery-3.1.1.min.js"></script>

  <!-- Bootstrap core CSS -->
  <link rel="stylesheet" href="/css/bootstrap.css">
  <link rel="stylesheet" href="/css/fashion.css">
  <link rel="stylesheet" href="/css/glyphs.css">

</head>
</html>


  <body data-spy="scroll" data-target="#toc" data-offset="50">


  


<header id="allheader" class="site-header" role="banner">
  <div class="clearfix container">
      <div class="site-branding">

          <h1 class="site-title">
            
              <a href="/" title="Cabin" rel="home"> Cabin </a>
            
          </h1>
          
          
            
          <nav id="main-navigation" class="main-navigation" role="navigation">
            <a class="nav-open">Menu</a>
            <a class="nav-close">Close</a>

            <div class="clearfix sf-menu">
              <ul id="main-nav" class="menu sf-js-enabled sf-arrows" style="touch-action: pan-y;">
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/">Home</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/archives">Archives</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/categories">Categories</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/tags">Tags</a> </li>
                    
                      <li class="menu-item menu-item-type-custom menu-item-object-custom menu-item-home menu-item-1663"> <a class="" href="/about">About</a> </li>
                    
              </ul>
            </div>
          </nav>

      </div>
  </div>
</header>


  <div id="container">
    <div id="wrap">
            
      <div id="content" class="outer">
        
          <section id="main" style="float:none;"><article id="post-Keras2" style="width: 66%; float:left;" class="article article-type-post" itemscope="" itemprop="blogPost">
  <div id="articleInner" class="clearfix post-1016 post type-post status-publish format-standard has-post-thumbnail hentry category-template-2 category-uncategorized tag-codex tag-edge-case tag-featured-image tag-image tag-template">
    
<div class="article-gallery">
  <div class="article-gallery-photos">
    
      <a class="article-gallery-img fancybox" href="/0.png" rel="gallery_cjrgap4i10001ffgc6xah3of8">
        <img src="/2019/01/28/Keras2//0.png" itemprop="image">
      </a>
    
  </div>
</div>

    
      <header class="article-header">
        
  
    <h1 class="thumb" itemprop="name">
      KERAS学习（二）：电影评级分类-二分类问题
    </h1>
  

      </header>
    
    <div class="article-meta">
      
	<a href="/2019/01/28/Keras2/" class="article-date">
	  <time datetime="2019-01-28T11:42:55.000Z" itemprop="datePublished">January 28, 2019</time>
	</a>

      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/Notes/">Notes</a>
 
      
	<span id="busuanzi_container_page_pv">
	  本文总阅读量<span id="busuanzi_value_page_pv"></span>次
	</span>

    </div>
    <div class="article-entry" itemprop="articleBody">
      
        <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#加载IMDB数据集</span></span><br><span class="line"><span class="keyword">from</span> keras.datasets <span class="keyword">import</span> imdb</span><br><span class="line">(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=<span class="number">10000</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Using TensorFlow backend.</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_data[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br></pre></td><td class="code"><pre><span class="line">[1,</span><br><span class="line"> 14,</span><br><span class="line"> 22,</span><br><span class="line"> 16,</span><br><span class="line"> 43,</span><br><span class="line"> 530,</span><br><span class="line"> 973,</span><br><span class="line"> 1622,</span><br><span class="line"> 1385,</span><br><span class="line"> 65,</span><br><span class="line"> 458,</span><br><span class="line"> 4468,</span><br><span class="line"> 66,</span><br><span class="line"> 3941,</span><br><span class="line"> 4,</span><br><span class="line"> 173,</span><br><span class="line"> 36,</span><br><span class="line"> 256,</span><br><span class="line"> 5,</span><br><span class="line"> 25,</span><br><span class="line"> 100,</span><br><span class="line"> 43,</span><br><span class="line"> 838,</span><br><span class="line"> 112,</span><br><span class="line"> 50,</span><br><span class="line"> 670,</span><br><span class="line"> 2,</span><br><span class="line"> 9,</span><br><span class="line"> 35,</span><br><span class="line"> 480,</span><br><span class="line"> 284,</span><br><span class="line"> 5,</span><br><span class="line"> 150,</span><br><span class="line"> 4,</span><br><span class="line"> 172,</span><br><span class="line"> 112,</span><br><span class="line"> 167,</span><br><span class="line"> 2,</span><br><span class="line"> 336,</span><br><span class="line"> 385,</span><br><span class="line"> 39,</span><br><span class="line"> 4,</span><br><span class="line"> 172,</span><br><span class="line"> 4536,</span><br><span class="line"> 1111,</span><br><span class="line"> 17,</span><br><span class="line"> 546,</span><br><span class="line"> 38,</span><br><span class="line"> 13,</span><br><span class="line"> 447,</span><br><span class="line"> 4,</span><br><span class="line"> 192,</span><br><span class="line"> 50,</span><br><span class="line"> 16,</span><br><span class="line"> 6,</span><br><span class="line"> 147,</span><br><span class="line"> 2025,</span><br><span class="line"> 19,</span><br><span class="line"> 14,</span><br><span class="line"> 22,</span><br><span class="line"> 4,</span><br><span class="line"> 1920,</span><br><span class="line"> 4613,</span><br><span class="line"> 469,</span><br><span class="line"> 4,</span><br><span class="line"> 22,</span><br><span class="line"> 71,</span><br><span class="line"> 87,</span><br><span class="line"> 12,</span><br><span class="line"> 16,</span><br><span class="line"> 43,</span><br><span class="line"> 530,</span><br><span class="line"> 38,</span><br><span class="line"> 76,</span><br><span class="line"> 15,</span><br><span class="line"> 13,</span><br><span class="line"> 1247,</span><br><span class="line"> 4,</span><br><span class="line"> 22,</span><br><span class="line"> 17,</span><br><span class="line"> 515,</span><br><span class="line"> 17,</span><br><span class="line"> 12,</span><br><span class="line"> 16,</span><br><span class="line"> 626,</span><br><span class="line"> 18,</span><br><span class="line"> 2,</span><br><span class="line"> 5,</span><br><span class="line"> 62,</span><br><span class="line"> 386,</span><br><span class="line"> 12,</span><br><span class="line"> 8,</span><br><span class="line"> 316,</span><br><span class="line"> 8,</span><br><span class="line"> 106,</span><br><span class="line"> 5,</span><br><span class="line"> 4,</span><br><span class="line"> 2223,</span><br><span class="line"> 5244,</span><br><span class="line"> 16,</span><br><span class="line"> 480,</span><br><span class="line"> 66,</span><br><span class="line"> 3785,</span><br><span class="line"> 33,</span><br><span class="line"> 4,</span><br><span class="line"> 130,</span><br><span class="line"> 12,</span><br><span class="line"> 16,</span><br><span class="line"> 38,</span><br><span class="line"> 619,</span><br><span class="line"> 5,</span><br><span class="line"> 25,</span><br><span class="line"> 124,</span><br><span class="line"> 51,</span><br><span class="line"> 36,</span><br><span class="line"> 135,</span><br><span class="line"> 48,</span><br><span class="line"> 25,</span><br><span class="line"> 1415,</span><br><span class="line"> 33,</span><br><span class="line"> 6,</span><br><span class="line"> 22,</span><br><span class="line"> 12,</span><br><span class="line"> 215,</span><br><span class="line"> 28,</span><br><span class="line"> 77,</span><br><span class="line"> 52,</span><br><span class="line"> 5,</span><br><span class="line"> 14,</span><br><span class="line"> 407,</span><br><span class="line"> 16,</span><br><span class="line"> 82,</span><br><span class="line"> 2,</span><br><span class="line"> 8,</span><br><span class="line"> 4,</span><br><span class="line"> 107,</span><br><span class="line"> 117,</span><br><span class="line"> 5952,</span><br><span class="line"> 15,</span><br><span class="line"> 256,</span><br><span class="line"> 4,</span><br><span class="line"> 2,</span><br><span class="line"> 7,</span><br><span class="line"> 3766,</span><br><span class="line"> 5,</span><br><span class="line"> 723,</span><br><span class="line"> 36,</span><br><span class="line"> 71,</span><br><span class="line"> 43,</span><br><span class="line"> 530,</span><br><span class="line"> 476,</span><br><span class="line"> 26,</span><br><span class="line"> 400,</span><br><span class="line"> 317,</span><br><span class="line"> 46,</span><br><span class="line"> 7,</span><br><span class="line"> 4,</span><br><span class="line"> 2,</span><br><span class="line"> 1029,</span><br><span class="line"> 13,</span><br><span class="line"> 104,</span><br><span class="line"> 88,</span><br><span class="line"> 4,</span><br><span class="line"> 381,</span><br><span class="line"> 15,</span><br><span class="line"> 297,</span><br><span class="line"> 98,</span><br><span class="line"> 32,</span><br><span class="line"> 2071,</span><br><span class="line"> 56,</span><br><span class="line"> 26,</span><br><span class="line"> 141,</span><br><span class="line"> 6,</span><br><span class="line"> 194,</span><br><span class="line"> 7486,</span><br><span class="line"> 18,</span><br><span class="line"> 4,</span><br><span class="line"> 226,</span><br><span class="line"> 22,</span><br><span class="line"> 21,</span><br><span class="line"> 134,</span><br><span class="line"> 476,</span><br><span class="line"> 26,</span><br><span class="line"> 480,</span><br><span class="line"> 5,</span><br><span class="line"> 144,</span><br><span class="line"> 30,</span><br><span class="line"> 5535,</span><br><span class="line"> 18,</span><br><span class="line"> 51,</span><br><span class="line"> 36,</span><br><span class="line"> 28,</span><br><span class="line"> 224,</span><br><span class="line"> 92,</span><br><span class="line"> 25,</span><br><span class="line"> 104,</span><br><span class="line"> 4,</span><br><span class="line"> 226,</span><br><span class="line"> 65,</span><br><span class="line"> 16,</span><br><span class="line"> 38,</span><br><span class="line"> 1334,</span><br><span class="line"> 88,</span><br><span class="line"> 12,</span><br><span class="line"> 16,</span><br><span class="line"> 283,</span><br><span class="line"> 5,</span><br><span class="line"> 16,</span><br><span class="line"> 4472,</span><br><span class="line"> 113,</span><br><span class="line"> 103,</span><br><span class="line"> 32,</span><br><span class="line"> 15,</span><br><span class="line"> 16,</span><br><span class="line"> 5345,</span><br><span class="line"> 19,</span><br><span class="line"> 178,</span><br><span class="line"> 32]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train_labels</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([1, 0, 0, ..., 0, 1, 0])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">max([max(sequence) <span class="keyword">for</span> sequence <span class="keyword">in</span> train_data])</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">9999</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#索引解码成单词</span></span><br><span class="line">word_index = imdb.get_word_index()</span><br><span class="line">reverse_word_index = dict(</span><br><span class="line">    [(value, key) <span class="keyword">for</span> (key, value) <span class="keyword">in</span> word_index.items()])</span><br><span class="line">decoded_review = <span class="string">' '</span>.join(</span><br><span class="line">    [reverse_word_index.get(i - <span class="number">3</span>, <span class="string">'?'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> train_data[<span class="number">0</span>]])  <span class="comment">#第一个评论</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">decoded_review</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;? this film was just brilliant casting location scenery story direction everyone&apos;s really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy&apos;s that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don&apos;t you think the whole story was so lovely because it was true and was someone&apos;s life after all that was shared with us all&quot;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#数据向量化</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">vectorize_sequences</span><span class="params">(sequences, dimension=<span class="number">10000</span>)</span>:</span></span><br><span class="line">    results= np.zeros((len(sequences), dimension))</span><br><span class="line">    <span class="keyword">for</span> i, sequence <span class="keyword">in</span> enumerate(sequences):</span><br><span class="line">        results[i, sequence] = <span class="number">1.</span></span><br><span class="line">    <span class="keyword">return</span> results</span><br><span class="line"></span><br><span class="line">x_train = vectorize_sequences(train_data)</span><br><span class="line">x_test = vectorize_sequences(test_data)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x_train[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array([0., 1., 1., ..., 0., 0., 0.])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#标签向量化</span></span><br><span class="line">y_train = np.asarray(train_labels).astype(<span class="string">'float32'</span>)</span><br><span class="line">y_test = np.asarray(test_labels).astype(<span class="string">'float32'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#构建网络 模型定义</span></span><br><span class="line"><span class="comment">#激活函数relu 所有负值归0</span></span><br><span class="line"><span class="comment">#任意值“压缩”到[0, 1]区间内</span></span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>, input_shape=(<span class="number">10000</span>,)))</span><br><span class="line">model.add(layers.Dense(<span class="number">16</span>, activation=<span class="string">'relu'</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>))</span><br><span class="line"><span class="comment">#relu 负值归零 sigmoid 任意值压缩到[0, 1]区间内</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#损失函数 优化器 指标 编译模型</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'accuracy'</span>])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#从训练数据中留出验证集</span></span><br><span class="line">x_val = x_train[:<span class="number">10000</span>]</span><br><span class="line">partial_x_train = x_train[<span class="number">10000</span>:]</span><br><span class="line"></span><br><span class="line">y_val = y_train[:<span class="number">10000</span>]</span><br><span class="line">partial_y_train = y_train[<span class="number">10000</span>:]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#训练模型</span></span><br><span class="line">model.compile(optimizer=<span class="string">'rmsprop'</span>, loss=<span class="string">'binary_crossentropy'</span>, metrics=[<span class="string">'acc'</span>])</span><br><span class="line">history = model.fit(partial_x_train, partial_y_train, epochs=<span class="number">20</span>, batch_size=<span class="number">512</span>, validation_data=(x_val, y_val))</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">Train on 15000 samples, validate on 10000 samples</span><br><span class="line">Epoch 1/20</span><br><span class="line">15000/15000 [==============================] - 6s 421us/step - loss: 0.5083 - acc: 0.7819 - val_loss: 0.3788 - val_acc: 0.8690</span><br><span class="line">Epoch 2/20</span><br><span class="line">15000/15000 [==============================] - 5s 311us/step - loss: 0.3001 - acc: 0.9048 - val_loss: 0.3000 - val_acc: 0.8901</span><br><span class="line">Epoch 3/20</span><br><span class="line">15000/15000 [==============================] - 4s 257us/step - loss: 0.2178 - acc: 0.9284 - val_loss: 0.3082 - val_acc: 0.8715</span><br><span class="line">Epoch 4/20</span><br><span class="line">15000/15000 [==============================] - 3s 221us/step - loss: 0.1750 - acc: 0.9435 - val_loss: 0.2838 - val_acc: 0.8839</span><br><span class="line">Epoch 5/20</span><br><span class="line">15000/15000 [==============================] - 4s 241us/step - loss: 0.1425 - acc: 0.9543 - val_loss: 0.2850 - val_acc: 0.8865</span><br><span class="line">Epoch 6/20</span><br><span class="line">15000/15000 [==============================] - 3s 222us/step - loss: 0.1149 - acc: 0.9652 - val_loss: 0.3163 - val_acc: 0.8773</span><br><span class="line">Epoch 7/20</span><br><span class="line">15000/15000 [==============================] - 4s 265us/step - loss: 0.0978 - acc: 0.9710 - val_loss: 0.3130 - val_acc: 0.8847</span><br><span class="line">Epoch 8/20</span><br><span class="line">15000/15000 [==============================] - 3s 231us/step - loss: 0.0807 - acc: 0.9765 - val_loss: 0.3861 - val_acc: 0.8653</span><br><span class="line">Epoch 9/20</span><br><span class="line">15000/15000 [==============================] - 3s 215us/step - loss: 0.0660 - acc: 0.9820 - val_loss: 0.3636 - val_acc: 0.8782</span><br><span class="line">Epoch 10/20</span><br><span class="line">15000/15000 [==============================] - 4s 254us/step - loss: 0.0555 - acc: 0.9852 - val_loss: 0.3845 - val_acc: 0.8790</span><br><span class="line">Epoch 11/20</span><br><span class="line">15000/15000 [==============================] - 3s 194us/step - loss: 0.0449 - acc: 0.9888 - val_loss: 0.4167 - val_acc: 0.8766</span><br><span class="line">Epoch 12/20</span><br><span class="line">15000/15000 [==============================] - 4s 247us/step - loss: 0.0385 - acc: 0.9913 - val_loss: 0.4511 - val_acc: 0.8700</span><br><span class="line">Epoch 13/20</span><br><span class="line">15000/15000 [==============================] - 4s 251us/step - loss: 0.0298 - acc: 0.9927 - val_loss: 0.4705 - val_acc: 0.8727</span><br><span class="line">Epoch 14/20</span><br><span class="line">15000/15000 [==============================] - 4s 259us/step - loss: 0.0244 - acc: 0.9949 - val_loss: 0.5029 - val_acc: 0.8723</span><br><span class="line">Epoch 15/20</span><br><span class="line">15000/15000 [==============================] - 3s 233us/step - loss: 0.0177 - acc: 0.9979 - val_loss: 0.5375 - val_acc: 0.8692</span><br><span class="line">Epoch 16/20</span><br><span class="line">15000/15000 [==============================] - 4s 239us/step - loss: 0.0170 - acc: 0.9967 - val_loss: 0.5728 - val_acc: 0.8702</span><br><span class="line">Epoch 17/20</span><br><span class="line">15000/15000 [==============================] - 3s 196us/step - loss: 0.0093 - acc: 0.9995 - val_loss: 0.6176 - val_acc: 0.8654</span><br><span class="line">Epoch 18/20</span><br><span class="line">15000/15000 [==============================] - 3s 224us/step - loss: 0.0117 - acc: 0.9975 - val_loss: 0.6386 - val_acc: 0.8669</span><br><span class="line">Epoch 19/20</span><br><span class="line">15000/15000 [==============================] - 3s 211us/step - loss: 0.0069 - acc: 0.9992 - val_loss: 0.7420 - val_acc: 0.8559</span><br><span class="line">Epoch 20/20</span><br><span class="line">15000/15000 [==============================] - 4s 238us/step - loss: 0.0043 - acc: 0.9999 - val_loss: 0.6976 - val_acc: 0.8655</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制训练损失和验证损失</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">history_dict = history.history</span><br><span class="line">loss_values = history_dict[<span class="string">'loss'</span>]</span><br><span class="line">val_loss_values = history_dict[<span class="string">'val_loss'</span>]</span><br><span class="line"></span><br><span class="line">epochs = range(<span class="number">1</span>, len(loss_values) + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss_values, <span class="string">'bo'</span>, label=<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(epochs, val_loss_values, <span class="string">'b'</span>, label=<span class="string">'Valifation loss'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Loss'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;Figure size 640x480 with 1 Axes&gt;</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">history_dict.keys()</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dict_keys([&apos;val_loss&apos;, &apos;val_acc&apos;, &apos;loss&apos;, &apos;acc&apos;])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#绘制训练精度和验证精度</span></span><br><span class="line">plt.clf()</span><br><span class="line">acc = history_dict[<span class="string">'acc'</span>]</span><br><span class="line">val_acc = history_dict[<span class="string">'val_acc'</span>]</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, acc, <span class="string">'bo'</span>, label=<span class="string">'Training acc'</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">'b'</span>, label=<span class="string">'Validation acc'</span>)</span><br><span class="line">plt.title(<span class="string">'Training and validation accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epochs'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2019/01/28/Keras2/output_15_0.png" alt="png"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">results = model.evaluate(x_test, y_test)</span><br><span class="line">results</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">25000/25000 [==============================] - 7s 272us/step</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[0.768154475197792, 0.85032]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#预测评价正面的可能性</span></span><br><span class="line">model.predict(x_test)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">array([[0.00997098],</span><br><span class="line">       [0.9999999 ],</span><br><span class="line">       [0.971289  ],</span><br><span class="line">       ...,</span><br><span class="line">       [0.00219277],</span><br><span class="line">       [0.0054981 ],</span><br><span class="line">       [0.72482127]], dtype=float32)</span><br></pre></td></tr></table></figure>

      
    </div>
    <footer class="entry-meta entry-footer">
      
	<span class="ico-folder"></span>
    <a class="article-category-link" href="/categories/Notes/">Notes</a>

      
  <span class="ico-tags"></span>
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Keras/">Keras</a></li></ul>

      
        
	<div id="comment">
		<!-- 来必力City版安装代码 -->
		<div id="lv-container" data-id="city" data-uid="MTAyMC8yOTQ4MS82MDQ5">
		<script type="text/javascript">
		   (function(d, s) {
		       var j, e = d.getElementsByTagName(s)[0];

		       if (typeof LivereTower === 'function') { return; }

		       j = d.createElement(s);
		       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
		       j.async = true;

		       e.parentNode.insertBefore(j, e);
		   })(document, 'script');
		</script>
		<noscript>为正常使用来必力评论功能请激活JavaScript</noscript>
		</div>
		<!-- City版安装代码已完成 -->
	</div>



      
    </footer>
    <hr class="entry-footer-hr">
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/01/28/Keras3/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          KERAS学习（三）：新闻分类-多分类问题
        
      </div>
    </a>
  
  
    <a href="/2019/01/27/Plugin1/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">Intellij IDEA插件开发入门（一）</div>
    </a>
  
</nav>

  
</article>

<!-- Table of Contents -->

  <aside id="sidebar">
    <div id="toc" class="toc-article">
    <strong class="toc-title">Contents</strong>
    
      <ol class="nav">None</ol>
    
    </div>
  </aside>
</section>
        
      </div>

    </div>
    <!-- <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
    <a href="/categories" class="mobile-nav-link">Categories</a>
  
    <a href="/tags" class="mobile-nav-link">Tags</a>
  
    <a href="/about" class="mobile-nav-link">About</a>
  
</nav> -->
    <footer id="footer" class="site-footer">
  

  <div class="clearfix container">
      <div class="site-info">
	      &copy; 2019 Cabin All Rights Reserved.
        
            <span id="busuanzi_container_site_uv">
              本站访客数<span id="busuanzi_value_site_uv"></span>人次  
              本站总访问量<span id="busuanzi_value_site_pv"></span>次
            </span>
          
      </div>
      <div class="site-credit">
        Theme by <a href="https://github.com/iTimeTraveler/hexo-theme-hipaper" target="_blank">hipaper</a>
      </div>
  </div>
</footer>


<!-- min height -->

<script>
    var wrapdiv = document.getElementById("wrap");
    var contentdiv = document.getElementById("content");

    wrapdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";
    contentdiv.style.minHeight = document.body.offsetHeight - document.getElementById("allheader").offsetHeight - document.getElementById("footer").offsetHeight + "px";


    <!-- headerblur min height -->
    
    
</script>
    
<div style="display: none;">
  <script src="https://s11.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
</div>

<!-- mathjax config similar to math.stackexchange -->

<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      processEscapes: true
    }
  });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
</script>

<script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
    });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>
<script src="/js/bootstrap.js"></script>
<script src="/js/main.js"></script>







  <div style="display: none;">
    <script src="https://s95.cnzz.com/z_stat.php?id=1260716016&web_id=1260716016" language="JavaScript"></script>
  </div>



	<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js">
	</script>






  </div>

  <a id="rocket" href="#top" class=""></a>
  <script type="text/javascript" src="/js/totop.js" async=""></script>
</body>
</html>
