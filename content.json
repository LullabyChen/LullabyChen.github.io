{"meta":{"title":"Cabin","subtitle":null,"description":null,"author":"Duo","url":"https://lullabychen.github.io"},"pages":[{"title":"About","date":"2019-01-24T04:02:05.000Z","updated":"2019-01-31T10:12:04.000Z","comments":false,"path":"about/index.html","permalink":"https://lullabychen.github.io/about/index.html","excerpt":"","text":"个人信息 Email: LullabyChen1104.gmail.com Github: LullabyChen"},{"title":"categories","date":"2019-01-24T04:04:56.000Z","updated":"2019-01-31T08:27:16.000Z","comments":false,"path":"categories/index.html","permalink":"https://lullabychen.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2019-01-24T04:04:04.000Z","updated":"2019-01-31T08:25:40.000Z","comments":false,"path":"tags/index.html","permalink":"https://lullabychen.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Java:idea打包jar包&部署到服务器过程全记录","slug":"Java-IDEA打jar包-部署到服务器过程全记录","date":"2019-10-09T12:54:24.000Z","updated":"2019-10-11T07:49:58.814Z","comments":true,"path":"2019/10/09/Java-IDEA打jar包-部署到服务器过程全记录/","link":"","permalink":"https://lullabychen.github.io/2019/10/09/Java-IDEA打jar包-部署到服务器过程全记录/","excerpt":"","text":"关于idea打包Spring boot为可执行jar包，并部署到服务器分为以下步骤： idea打包jar包 tmux开会话 连接远程服务器 上传文件 成功运行 1. idea打包jar包我尝试了两种方式：分别是用idea自带的打包形式，和maven管理工具打包。 ####1.1. 用idea自带的打包形式 可能存在的问题： JAR包运行错误：“Error ：Invalid or corrupt jarfile xxx.jar”。（已解决Q1） 有些项目依赖没有打进jar包，在项目目录下可以运行，但单独jar包不能运行。（未解决Q2） 主菜单栏File -&gt; Project Structure -&gt; Artifacts -&gt; +(Add) -&gt; JAR -&gt; From modules with dependencies… -&gt; 选择Main Class -&gt; 选择copy to the output… -&gt; 选择META-INF生成目录(注意放在项目目录下，不要放在默认目录，解决Q1) -&gt; OK -&gt; (尝试了创建libs目录，放第三方jar包，依然没有解决Q2) -&gt; OK -&gt; 主菜单栏Build -&gt; Build Artifacts… -&gt; build生成jar包。 最终结果：在该目录下java -jar xxx.jar可以运行，单独放在服务器报错。 ####1.2. 用自带的maven管理工具打包 于是尝试第二种方法。作为一个spring boot项目，用idea自带的maven管理工具进行打包。步骤如下图所示，依次点击clean、compile、package和install。 获得如下图文件夹。此刻的jar包可以独立运行，没有依赖缺失的问题。 2. tmux开会话命令介绍：Tmux (Terminal Multiplexer)是一款终端复用软件，使用它的好处一是，通过一个终端登录远程主机并运行tmux后，在其中可以开启多个控制台而无需再“浪费”多余的终端来连接这台远程主机，二是当终端关闭后该shell里面运行的任务进程也会随之中断，通过使用tmux就能很容易的解决这个问题。 用tmux开一个会话，防止断开服务器连接后应用停止运行。如下为步骤及常见操作，均在命令行操作： 在mac中安装tmux 1bre install tmux 修改快捷键前缀（此处没用） 当想要使用tmux快捷键时，需要先按下快捷键前缀，然后再按下快捷键。由于键盘上Ctrl-b太远，所以修改为Ctrl-a。 将以下配置加入到tmux的配置文件~/.tmux.conf中（没有就创建一个） 12unbind C-bset -g prefix C-a 创建新的会话 1tmux new -s &lt;name-of-my-session&gt; 获取会话列表 1tmux ls 在会话外进入会话 123tmux attach -t &lt;name-of-my-session&gt;#ortmux a -t &lt;name-of-my-session&gt; 可能会产生报错： 1sessions should be nested with care, unset $TMUX to force 解决办法： 1unset TMUX 然后再执行tmux attach。 回到之前的会话 1tmux attach 临时退出但不删除会话 Ctrl+a，然后d（注意先后顺序，并非同时按下） 退出并删除会话 Ctrl+a，然后x 3. 连接远程服务器前提：有需要连接的服务器的ip、账户、密码。 打开终端，进入根目录 1sudo su - 连接服务器（输入用户名和地址） 1ssh server-username@remote-ip 如果第一次连接需要输入yes或者no确认是否连接，输入yes回车。 输入密码 连接成功 进行操作 假如要部署，首先应该查看端口占用情况： 1234#查看服务器所有端口netstat -ntlp#查看服务器指定端口是否被占用lsof -i:8080 退出服务器 1exit 4. 上传文件我采用的SCP方式传输文件。 上传文件 12scp local-file-url server-username@remote-ip:remote-file-url#上传文件夹加-r参数 下载文件 1scp -r server-username@remote-ip:remote-file-url local-file-url 另外mac平台下可以使用sz和rz命令进行远程服务器文件的上传下载，似乎需要本机和服务器都安装lrzsz。 5. 成功运行配置合适的端口，jar包上传到服务器之后，就可以执行啦。 1java -jar xxxxx.jar 成功执行之后，Ctrl+a|d临时退出会话，就可以一直挂着任务进程了。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://lullabychen.github.io/categories/Notes/"}],"tags":[{"name":"Java","slug":"Java","permalink":"https://lullabychen.github.io/tags/Java/"}]},{"title":"Mysql：版本修改","slug":"Mysql：版本修改","date":"2019-09-03T07:59:51.000Z","updated":"2019-09-03T09:49:01.070Z","comments":true,"path":"2019/09/03/Mysql：版本修改/","link":"","permalink":"https://lullabychen.github.io/2019/09/03/Mysql：版本修改/","excerpt":"","text":"背景：使用homebrew先后下载了8.0版本和5.7版本的mysql，但是项目要求使用5.7版本的mysql。","categories":[{"name":"Notes","slug":"Notes","permalink":"https://lullabychen.github.io/categories/Notes/"}],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://lullabychen.github.io/tags/Mysql/"}]},{"title":"Markdown：空格","slug":"Markdown空格","date":"2019-04-05T07:19:24.000Z","updated":"2019-09-03T09:38:51.098Z","comments":true,"path":"2019/04/05/Markdown空格/","link":"","permalink":"https://lullabychen.github.io/2019/04/05/Markdown空格/","excerpt":"","text":"半角空格 &amp;ensp; 全角空格 &amp;emsp;","categories":[{"name":"Notes","slug":"Notes","permalink":"https://lullabychen.github.io/categories/Notes/"}],"tags":[{"name":"Markdown","slug":"Markdown","permalink":"https://lullabychen.github.io/tags/Markdown/"}]},{"title":"KERAS学习（四）：预测房价-回归问题","slug":"Keras4","date":"2019-01-28T12:08:52.000Z","updated":"2019-01-28T12:16:36.000Z","comments":true,"path":"2019/01/28/Keras4/","link":"","permalink":"https://lullabychen.github.io/2019/01/28/Keras4/","excerpt":"123#加载波士顿房价数据from keras.datasets import boston_housing(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data() 1Using TensorFlow backend. 12Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz57344/57026 [==============================] - 1s 10us/step","text":"123#加载波士顿房价数据from keras.datasets import boston_housing(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data() 1Using TensorFlow backend. 12Downloading data from https://s3.amazonaws.com/keras-datasets/boston_housing.npz57344/57026 [==============================] - 1s 10us/step 1train_data.shape 1(404, 13) 1test_data.shape 1(102, 13) 1train_targets 12345678910111213141516171819202122232425262728293031323334353637array([15.2, 42.3, 50. , 21.1, 17.7, 18.5, 11.3, 15.6, 15.6, 14.4, 12.1, 17.9, 23.1, 19.9, 15.7, 8.8, 50. , 22.5, 24.1, 27.5, 10.9, 30.8, 32.9, 24. , 18.5, 13.3, 22.9, 34.7, 16.6, 17.5, 22.3, 16.1, 14.9, 23.1, 34.9, 25. , 13.9, 13.1, 20.4, 20. , 15.2, 24.7, 22.2, 16.7, 12.7, 15.6, 18.4, 21. , 30.1, 15.1, 18.7, 9.6, 31.5, 24.8, 19.1, 22. , 14.5, 11. , 32. , 29.4, 20.3, 24.4, 14.6, 19.5, 14.1, 14.3, 15.6, 10.5, 6.3, 19.3, 19.3, 13.4, 36.4, 17.8, 13.5, 16.5, 8.3, 14.3, 16. , 13.4, 28.6, 43.5, 20.2, 22. , 23. , 20.7, 12.5, 48.5, 14.6, 13.4, 23.7, 50. , 21.7, 39.8, 38.7, 22.2, 34.9, 22.5, 31.1, 28.7, 46. , 41.7, 21. , 26.6, 15. , 24.4, 13.3, 21.2, 11.7, 21.7, 19.4, 50. , 22.8, 19.7, 24.7, 36.2, 14.2, 18.9, 18.3, 20.6, 24.6, 18.2, 8.7, 44. , 10.4, 13.2, 21.2, 37. , 30.7, 22.9, 20. , 19.3, 31.7, 32. , 23.1, 18.8, 10.9, 50. , 19.6, 5. , 14.4, 19.8, 13.8, 19.6, 23.9, 24.5, 25. , 19.9, 17.2, 24.6, 13.5, 26.6, 21.4, 11.9, 22.6, 19.6, 8.5, 23.7, 23.1, 22.4, 20.5, 23.6, 18.4, 35.2, 23.1, 27.9, 20.6, 23.7, 28. , 13.6, 27.1, 23.6, 20.6, 18.2, 21.7, 17.1, 8.4, 25.3, 13.8, 22.2, 18.4, 20.7, 31.6, 30.5, 20.3, 8.8, 19.2, 19.4, 23.1, 23. , 14.8, 48.8, 22.6, 33.4, 21.1, 13.6, 32.2, 13.1, 23.4, 18.9, 23.9, 11.8, 23.3, 22.8, 19.6, 16.7, 13.4, 22.2, 20.4, 21.8, 26.4, 14.9, 24.1, 23.8, 12.3, 29.1, 21. , 19.5, 23.3, 23.8, 17.8, 11.5, 21.7, 19.9, 25. , 33.4, 28.5, 21.4, 24.3, 27.5, 33.1, 16.2, 23.3, 48.3, 22.9, 22.8, 13.1, 12.7, 22.6, 15. , 15.3, 10.5, 24. , 18.5, 21.7, 19.5, 33.2, 23.2, 5. , 19.1, 12.7, 22.3, 10.2, 13.9, 16.3, 17. , 20.1, 29.9, 17.2, 37.3, 45.4, 17.8, 23.2, 29. , 22. , 18. , 17.4, 34.6, 20.1, 25. , 15.6, 24.8, 28.2, 21.2, 21.4, 23.8, 31. , 26.2, 17.4, 37.9, 17.5, 20. , 8.3, 23.9, 8.4, 13.8, 7.2, 11.7, 17.1, 21.6, 50. , 16.1, 20.4, 20.6, 21.4, 20.6, 36.5, 8.5, 24.8, 10.8, 21.9, 17.3, 18.9, 36.2, 14.9, 18.2, 33.3, 21.8, 19.7, 31.6, 24.8, 19.4, 22.8, 7.5, 44.8, 16.8, 18.7, 50. , 50. , 19.5, 20.1, 50. , 17.2, 20.8, 19.3, 41.3, 20.4, 20.5, 13.8, 16.5, 23.9, 20.6, 31.5, 23.3, 16.8, 14. , 33.8, 36.1, 12.8, 18.3, 18.7, 19.1, 29. , 30.1, 50. , 50. , 22. , 11.9, 37.6, 50. , 22.7, 20.8, 23.5, 27.9, 50. , 19.3, 23.9, 22.6, 15.2, 21.7, 19.2, 43.8, 20.3, 33.2, 19.9, 22.5, 32.7, 22. , 17.1, 19. , 15. , 16.1, 25.1, 23.7, 28.7, 37.2, 22.6, 16.4, 25. , 29.8, 22.1, 17.4, 18.1, 30.3, 17.5, 24.7, 12.6, 26.5, 28.7, 13.3, 10.4, 24.4, 23. , 20. , 17.8, 7. , 11.8, 24.4, 13.8, 19.4, 25.2, 19.4, 19.4, 29.1]) 12345678#数据特征标准化 （特征-平均值）/标准差mean = train_data.mean(axis=0) #特征平均值train_data -= meanstd = train_data.std(axis=0) #标准差train_data /= stdtest_data -= meantest_data /= std 1234567891011121314#构建网络 模型定义from keras import modelsfrom keras import layers#需要将同一个模型多次实例化，所以用一个函数来构建模型#MSE损失函数（mean squared error）：均方误差，预测值与目标值之差的平方#MAE指标（mean absolute error）：平均绝对误差，预测值与目标值之差的绝对值def build_model(): model = models.Sequential() model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1], ))) model.add(layers.Dense(64, activation='relu')) model.add(layers.Dense(1)) #没有激活，线性层 model.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) return model 1234567891011121314151617181920212223242526272829303132#K折交叉验证import numpy as npk = 4num_val_samples = len(train_data) // knum_epochs = 100 #训练100轮次all_scores = []for i in range(k): print('processing fold #', i) #准备验证数据：第k个分区的数据 val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples] val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples] #准备训练数据：其他所有分区的数据 partial_train_data = np.concatenate( [train_data[:i * num_val_samples], train_data[(i + 1) * num_val_samples: ]], axis=0 ) partial_train_targets = np.concatenate( [train_targets[:i * num_val_samples], train_targets[(i + 1) * num_val_samples: ]], axis=0 ) #构建Keras模型（已编译） model = build_model() #训练模型（静默模式，verbose=0） model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=1, verbose=0) 在验证数据上评估模型 val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0) all_scores.append(val_mae) 1234processing fold # 0processing fold # 1processing fold # 2processing fold # 3 1all_scores 1[1.953495462342064, 2.7316349308089456, 2.4950007542525188, 2.304117675464932] 12np.mean(all_scores)#差别很大 12.371062205717115 123456789101112131415161718192021222324252627#重新训练 500轮次#保存每折的验证结果num_epochs = 500all_mae_histories = []for i in range(k): print('processing fold #', i) val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples] val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples] partial_train_data = np.concatenate( [train_data[:i * num_val_samples], train_data[(i + 1) * num_val_samples: ]], axis=0 ) partial_train_targets = np.concatenate( [train_targets[:i * num_val_samples], train_targets[(i + 1) * num_val_samples: ]], axis=0 ) model = build_model() history = model.fit(partial_train_data, partial_train_targets, validation_data=(val_data, val_targets), epochs=num_epochs, batch_size=1, verbose=0) mae_history = history.history['val_mean_absolute_error'] all_mae_histories.append(mae_history) 1234processing fold # 0processing fold # 1processing fold # 2processing fold # 3 123#计算所有轮次中的K折验证分数平均值average_mae_history = [ np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)] 1234567#绘制验证分数 纵轴范围较大，数据方差相对较大，难以看清规律，重新绘制import matplotlib.pyplot as pltplt.plot(range(1, len(average_mae_history) + 1), average_mae_history)plt.xlabel('Epochs')plt.ylabel('Validation MAE')plt.show() 1234567891011121314151617#重新绘制：删除前10个数据点，将每个数据点替换为前面数据点的指数移动平均值def smooth_curve(points, factor=0.9): smoothed_points = [] for point in points: if smoothed_points: previous = smoothed_points[-1] smoothed_points.append(previous * factor + point * (1 - factor)) else: smoothed_points.append(point) return smoothed_pointssmooth_mae_history = smooth_curve(average_mae_history[10: ])plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)plt.xlabel('Epochs')plt.ylabel('Validation MAE')plt.show() 12345#上一轮出现过拟合#训练最终模型model = build_model()model.fit(train_data, train_targets, epochs=80, batch_size=16, verbose=0)test_mse_score, test_mae_score = model.evaluate(test_data, test_targets) 1102/102 [==============================] - 0s 2ms/step 12#最终结果test_mae_score 12.577465954948874 1test_mse_score 116.48435390696806 12","categories":[{"name":"Notes","slug":"Notes","permalink":"https://lullabychen.github.io/categories/Notes/"}],"tags":[{"name":"Keras","slug":"Keras","permalink":"https://lullabychen.github.io/tags/Keras/"}]},{"title":"KERAS学习（三）：新闻分类-多分类问题","slug":"Keras3","date":"2019-01-28T12:08:44.000Z","updated":"2019-01-28T12:16:57.000Z","comments":true,"path":"2019/01/28/Keras3/","link":"","permalink":"https://lullabychen.github.io/2019/01/28/Keras3/","excerpt":"1234#单标签 多分类#加载路透社数据集from keras.datasets import reuters(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000) 1Using TensorFlow backend. 12Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz2113536/2110848 [==============================] - 7s 3us/step","text":"1234#单标签 多分类#加载路透社数据集from keras.datasets import reuters(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000) 1Using TensorFlow backend. 12Downloading data from https://s3.amazonaws.com/text-datasets/reuters.npz2113536/2110848 [==============================] - 7s 3us/step 1len(train_data) 18982 1len(test_data) 12246 1train_data[10] 12345678910111213141516171819202122232425262728293031[1, 245, 273, 207, 156, 53, 74, 160, 26, 14, 46, 296, 26, 39, 74, 2979, 3554, 14, 46, 4689, 4329, 86, 61, 3499, 4795, 14, 61, 451, 4329, 17, 12] 12345#将索引解码为新闻文本word_index = reuters.get_word_index()reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])decoded_newswire = ' '.join([reverse_word_index.get(i - 3, '?') for i in train_data[0]])#第一个新闻 12Downloading data from https://s3.amazonaws.com/text-datasets/reuters_word_index.json557056/550378 [==============================] - 7s 13us/step 1train_labels[0] 13 123456789#数据向量化import numpy as npdef vectorize_sequences(sequences, dimension=10000): results = np.zeros((len(sequences), dimension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1 return resultsx_train = vectorize_sequences(train_data)x_test = vectorize_sequences(test_data) 123456789#标签向量化def to_one_hot(labels, dimension=46): results = np.zeros((len(labels), dimension)) for i, label in enumerate(labels): results[i, label] = 1 return resultsone_hot_train_labels = to_one_hot(train_labels)one_hot_test_labels = to_one_hot(test_labels) 123456789#构建网络 模型定义#激活函数 输出在46个不同输出类别上的概率分布from keras import modelsfrom keras import layersmodel = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(46, activation='softmax')) 123#编译模型#分类交叉熵 衡量两个概率分布之间的距离model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) 123456#留出验证集x_val = x_train[:1000]partial_x_train = x_train[1000:]y_val = one_hot_train_labels[:1000]partial_y_train = one_hot_train_labels[1000:] 12#训练模型history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val)) 1234567891011121314151617181920212223242526272829303132333435363738394041Train on 7982 samples, validate on 1000 samplesEpoch 1/207982/7982 [==============================] - 4s 452us/step - loss: 2.4979 - acc: 0.4915 - val_loss: 1.6824 - val_acc: 0.6470Epoch 2/207982/7982 [==============================] - 2s 264us/step - loss: 1.3942 - acc: 0.7045 - val_loss: 1.2821 - val_acc: 0.7160Epoch 3/207982/7982 [==============================] - 2s 212us/step - loss: 1.0509 - acc: 0.7686 - val_loss: 1.1187 - val_acc: 0.7640Epoch 4/207982/7982 [==============================] - 1s 175us/step - loss: 0.8256 - acc: 0.8281 - val_loss: 1.0233 - val_acc: 0.7740Epoch 5/207982/7982 [==============================] - 1s 175us/step - loss: 0.6607 - acc: 0.8639 - val_loss: 0.9730 - val_acc: 0.7940Epoch 6/207982/7982 [==============================] - 1s 188us/step - loss: 0.5260 - acc: 0.8928 - val_loss: 0.9196 - val_acc: 0.8100Epoch 7/207982/7982 [==============================] - 1s 183us/step - loss: 0.4292 - acc: 0.9116 - val_loss: 0.9124 - val_acc: 0.8030Epoch 8/207982/7982 [==============================] - 2s 201us/step - loss: 0.3498 - acc: 0.9270 - val_loss: 0.8942 - val_acc: 0.8170Epoch 9/207982/7982 [==============================] - 2s 194us/step - loss: 0.2894 - acc: 0.9385 - val_loss: 0.9147 - val_acc: 0.8060Epoch 10/207982/7982 [==============================] - 1s 181us/step - loss: 0.2451 - acc: 0.9453 - val_loss: 0.9124 - val_acc: 0.8130Epoch 11/207982/7982 [==============================] - 1s 175us/step - loss: 0.2102 - acc: 0.9484 - val_loss: 0.9480 - val_acc: 0.8120Epoch 12/207982/7982 [==============================] - 1s 173us/step - loss: 0.1881 - acc: 0.9523 - val_loss: 0.9600 - val_acc: 0.8050Epoch 13/207982/7982 [==============================] - 2s 189us/step - loss: 0.1662 - acc: 0.9526 - val_loss: 0.9964 - val_acc: 0.7960Epoch 14/207982/7982 [==============================] - 2s 229us/step - loss: 0.1530 - acc: 0.9550 - val_loss: 0.9766 - val_acc: 0.8050Epoch 15/207982/7982 [==============================] - 2s 194us/step - loss: 0.1457 - acc: 0.9548 - val_loss: 1.0243 - val_acc: 0.7970Epoch 16/207982/7982 [==============================] - 1s 176us/step - loss: 0.1332 - acc: 0.9553 - val_loss: 1.0392 - val_acc: 0.8020Epoch 17/207982/7982 [==============================] - 1s 171us/step - loss: 0.1260 - acc: 0.9553 - val_loss: 1.0396 - val_acc: 0.7980Epoch 18/207982/7982 [==============================] - 1s 183us/step - loss: 0.1169 - acc: 0.9560 - val_loss: 1.0390 - val_acc: 0.8150Epoch 19/207982/7982 [==============================] - 1s 182us/step - loss: 0.1165 - acc: 0.9569 - val_loss: 1.0301 - val_acc: 0.8090Epoch 20/207982/7982 [==============================] - 1s 177us/step - loss: 0.1140 - acc: 0.9580 - val_loss: 1.0484 - val_acc: 0.8030 12345678910111213141516#绘制训练损失和验证损失import matplotlib.pyplot as pltloss = history.history['loss']val_loss = history.history['val_loss']epochs = range(1, len(loss) + 1)plt.plot(epochs, loss, 'bo', label='Training loss')plt.plot(epochs, val_loss, 'b', label='Validation loss')plt.title('Training and validation loss')plt.xlabel('Epochs')plt.ylabel('Loss')plt.legend()plt.show() 12345678910111213plt.clf()acc = history.history['acc']val_acc = history.history['val_acc']plt.plot(epochs, acc, 'bo', label='Training acc')plt.plot(epochs, val_acc, 'b', label='Validation acc')plt.title('Training and validation accuracy')plt.xlabel('Epochs')plt.ylabel('Accuracy')plt.legend()plt.show() 12345678model = models.Sequential()model.add(layers.Dense(64, activation='relu', input_shape=(10000,)))model.add(layers.Dense(64, activation='relu'))model.add(layers.Dense(46, activation='softmax'))model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])history = model.fit(partial_x_train, partial_y_train, epochs=9, batch_size=512, validation_data=(x_val, y_val))results = model.evaluate(x_test, one_hot_test_labels) 1234567891011121314151617181920Train on 7982 samples, validate on 1000 samplesEpoch 1/97982/7982 [==============================] - 4s 494us/step - loss: 2.5416 - acc: 0.5223 - val_loss: 1.6816 - val_acc: 0.6490Epoch 2/97982/7982 [==============================] - 2s 271us/step - loss: 1.3779 - acc: 0.7096 - val_loss: 1.2817 - val_acc: 0.7240Epoch 3/97982/7982 [==============================] - 2s 225us/step - loss: 1.0201 - acc: 0.7783 - val_loss: 1.1325 - val_acc: 0.7500Epoch 4/97982/7982 [==============================] - 1s 182us/step - loss: 0.8026 - acc: 0.8237 - val_loss: 1.0541 - val_acc: 0.7580Epoch 5/97982/7982 [==============================] - 1s 180us/step - loss: 0.6429 - acc: 0.8617 - val_loss: 0.9743 - val_acc: 0.8000Epoch 6/97982/7982 [==============================] - 2s 191us/step - loss: 0.5152 - acc: 0.8933 - val_loss: 0.9122 - val_acc: 0.8130Epoch 7/97982/7982 [==============================] - 1s 174us/step - loss: 0.4152 - acc: 0.9138 - val_loss: 0.8975 - val_acc: 0.8240Epoch 8/97982/7982 [==============================] - 1s 170us/step - loss: 0.3377 - acc: 0.9276 - val_loss: 0.8781 - val_acc: 0.8240Epoch 9/97982/7982 [==============================] - 2s 193us/step - loss: 0.2803 - acc: 0.9369 - val_loss: 0.9426 - val_acc: 0.80202246/2246 [==============================] - 1s 354us/step 1results 1[1.0234324284567964, 0.7782724844698171] 12#在新数据上生成预测结果predictions = model.predict(x_test) 1predictions[0].shape 1(46,) 1np.sum(predictions[0]) 10.99999994 1np.argmax(predictions[0]) 13 1predictions[0] 123456789101112array([9.00706073e-06, 1.12914335e-04, 3.39240214e-05, 9.69965935e-01, 1.83314960e-02, 1.24670180e-07, 7.63048884e-05, 3.95889074e-05, 3.83749488e-03, 2.36399887e-06, 3.72867580e-05, 9.80967656e-04, 5.15155261e-05, 2.07923913e-05, 5.04263471e-06, 1.48046838e-05, 1.14349602e-03, 1.92527470e-04, 3.11047770e-04, 1.02656987e-03, 1.14359299e-03, 5.38451481e-04, 4.49636536e-06, 7.14166526e-05, 9.90290209e-06, 2.55455700e-04, 2.65549829e-06, 2.30972691e-05, 5.41368354e-06, 1.46480859e-04, 2.82360037e-04, 1.54729976e-04, 1.18201979e-05, 5.56262385e-05, 4.05374340e-05, 1.80534789e-05, 1.31222856e-04, 5.19382884e-05, 1.25783903e-04, 2.72022764e-04, 3.01298915e-05, 3.90922709e-04, 1.79568303e-06, 2.20499132e-05, 6.57706141e-06, 1.01889082e-05], dtype=float32) 1predictions[0][3] 10.96996593 12","categories":[{"name":"Notes","slug":"Notes","permalink":"https://lullabychen.github.io/categories/Notes/"}],"tags":[{"name":"Keras","slug":"Keras","permalink":"https://lullabychen.github.io/tags/Keras/"}]},{"title":"KERAS学习（二）：电影评级分类-二分类问题","slug":"Keras2","date":"2019-01-28T11:42:55.000Z","updated":"2019-02-18T11:20:27.000Z","comments":true,"path":"2019/01/28/Keras2/","link":"","permalink":"https://lullabychen.github.io/2019/01/28/Keras2/","excerpt":"123#加载IMDB数据集from keras.datasets import imdb(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) 1Using TensorFlow backend.","text":"123#加载IMDB数据集from keras.datasets import imdb(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) 1Using TensorFlow backend. 1train_data[0] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32] 1train_labels 1array([1, 0, 0, ..., 0, 1, 0]) 1max([max(sequence) for sequence in train_data]) 19999 123456#索引解码成单词word_index = imdb.get_word_index()reverse_word_index = dict( [(value, key) for (key, value) in word_index.items()])decoded_review = ' '.join( [reverse_word_index.get(i - 3, '?') for i in train_data[0]]) #第一个评论 1decoded_review 1&quot;? this film was just brilliant casting location scenery story direction everyone&apos;s really suited the part they played and you could just imagine being there robert ? is an amazing actor and now the same being director ? father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for ? and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also ? to the two little boy&apos;s that played the ? of norman and paul they were just brilliant children are often left out of the ? list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don&apos;t you think the whole story was so lovely because it was true and was someone&apos;s life after all that was shared with us all&quot; 12345678910#数据向量化import numpy as npdef vectorize_sequences(sequences, dimension=10000): results= np.zeros((len(sequences), dimension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. return resultsx_train = vectorize_sequences(train_data)x_test = vectorize_sequences(test_data) 1x_train[0] 1array([0., 1., 1., ..., 0., 0., 0.]) 123#标签向量化y_train = np.asarray(train_labels).astype('float32')y_test = np.asarray(test_labels).astype('float32') 12345678910#构建网络 模型定义#激活函数relu 所有负值归0#任意值“压缩”到[0, 1]区间内from keras import modelsfrom keras import layersmodel = models.Sequential()model.add(layers.Dense(16, activation='relu', input_shape=(10000,)))model.add(layers.Dense(16, activation='relu'))model.add(layers.Dense(1, activation='sigmoid'))#relu 负值归零 sigmoid 任意值压缩到[0, 1]区间内 123#损失函数 优化器 指标 编译模型#model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) 123456#从训练数据中留出验证集x_val = x_train[:10000]partial_x_train = x_train[10000:]y_val = y_train[:10000]partial_y_train = y_train[10000:] 123#训练模型model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val)) 1234567891011121314151617181920212223242526272829303132333435363738394041Train on 15000 samples, validate on 10000 samplesEpoch 1/2015000/15000 [==============================] - 6s 421us/step - loss: 0.5083 - acc: 0.7819 - val_loss: 0.3788 - val_acc: 0.8690Epoch 2/2015000/15000 [==============================] - 5s 311us/step - loss: 0.3001 - acc: 0.9048 - val_loss: 0.3000 - val_acc: 0.8901Epoch 3/2015000/15000 [==============================] - 4s 257us/step - loss: 0.2178 - acc: 0.9284 - val_loss: 0.3082 - val_acc: 0.8715Epoch 4/2015000/15000 [==============================] - 3s 221us/step - loss: 0.1750 - acc: 0.9435 - val_loss: 0.2838 - val_acc: 0.8839Epoch 5/2015000/15000 [==============================] - 4s 241us/step - loss: 0.1425 - acc: 0.9543 - val_loss: 0.2850 - val_acc: 0.8865Epoch 6/2015000/15000 [==============================] - 3s 222us/step - loss: 0.1149 - acc: 0.9652 - val_loss: 0.3163 - val_acc: 0.8773Epoch 7/2015000/15000 [==============================] - 4s 265us/step - loss: 0.0978 - acc: 0.9710 - val_loss: 0.3130 - val_acc: 0.8847Epoch 8/2015000/15000 [==============================] - 3s 231us/step - loss: 0.0807 - acc: 0.9765 - val_loss: 0.3861 - val_acc: 0.8653Epoch 9/2015000/15000 [==============================] - 3s 215us/step - loss: 0.0660 - acc: 0.9820 - val_loss: 0.3636 - val_acc: 0.8782Epoch 10/2015000/15000 [==============================] - 4s 254us/step - loss: 0.0555 - acc: 0.9852 - val_loss: 0.3845 - val_acc: 0.8790Epoch 11/2015000/15000 [==============================] - 3s 194us/step - loss: 0.0449 - acc: 0.9888 - val_loss: 0.4167 - val_acc: 0.8766Epoch 12/2015000/15000 [==============================] - 4s 247us/step - loss: 0.0385 - acc: 0.9913 - val_loss: 0.4511 - val_acc: 0.8700Epoch 13/2015000/15000 [==============================] - 4s 251us/step - loss: 0.0298 - acc: 0.9927 - val_loss: 0.4705 - val_acc: 0.8727Epoch 14/2015000/15000 [==============================] - 4s 259us/step - loss: 0.0244 - acc: 0.9949 - val_loss: 0.5029 - val_acc: 0.8723Epoch 15/2015000/15000 [==============================] - 3s 233us/step - loss: 0.0177 - acc: 0.9979 - val_loss: 0.5375 - val_acc: 0.8692Epoch 16/2015000/15000 [==============================] - 4s 239us/step - loss: 0.0170 - acc: 0.9967 - val_loss: 0.5728 - val_acc: 0.8702Epoch 17/2015000/15000 [==============================] - 3s 196us/step - loss: 0.0093 - acc: 0.9995 - val_loss: 0.6176 - val_acc: 0.8654Epoch 18/2015000/15000 [==============================] - 3s 224us/step - loss: 0.0117 - acc: 0.9975 - val_loss: 0.6386 - val_acc: 0.8669Epoch 19/2015000/15000 [==============================] - 3s 211us/step - loss: 0.0069 - acc: 0.9992 - val_loss: 0.7420 - val_acc: 0.8559Epoch 20/2015000/15000 [==============================] - 4s 238us/step - loss: 0.0043 - acc: 0.9999 - val_loss: 0.6976 - val_acc: 0.8655 1234567891011121314151617#绘制训练损失和验证损失import matplotlib.pyplot as plthistory_dict = history.historyloss_values = history_dict['loss']val_loss_values = history_dict['val_loss']epochs = range(1, len(loss_values) + 1)plt.plot(epochs, loss_values, 'bo', label='Training loss')plt.plot(epochs, val_loss_values, 'b', label='Valifation loss')plt.title('Training and validation loss')plt.xlabel('Epochs')plt.ylabel('Loss')plt.legend()plt.show() 1&lt;Figure size 640x480 with 1 Axes&gt; 1history_dict.keys() 1dict_keys([&apos;val_loss&apos;, &apos;val_acc&apos;, &apos;loss&apos;, &apos;acc&apos;]) 12345678910111213#绘制训练精度和验证精度plt.clf()acc = history_dict['acc']val_acc = history_dict['val_acc']plt.plot(epochs, acc, 'bo', label='Training acc')plt.plot(epochs, val_acc, 'b', label='Validation acc')plt.title('Training and validation accuracy')plt.xlabel('Epochs')plt.ylabel('Accuracy')plt.legend()plt.show() 12results = model.evaluate(x_test, y_test)results 125000/25000 [==============================] - 7s 272us/step 1[0.768154475197792, 0.85032] 12#预测评价正面的可能性model.predict(x_test) 1234567array([[0.00997098], [0.9999999 ], [0.971289 ], ..., [0.00219277], [0.0054981 ], [0.72482127]], dtype=float32)","categories":[{"name":"Notes","slug":"Notes","permalink":"https://lullabychen.github.io/categories/Notes/"}],"tags":[{"name":"Keras","slug":"Keras","permalink":"https://lullabychen.github.io/tags/Keras/"}]},{"title":"Intellij IDEA插件开发入门（一）","slug":"Plugin1","date":"2019-01-27T13:13:10.000Z","updated":"2019-01-28T09:28:47.000Z","comments":true,"path":"2019/01/27/Plugin1/","link":"","permalink":"https://lullabychen.github.io/2019/01/27/Plugin1/","excerpt":"Intellij IDEA插件开发有两种方式： Gradle Plugin Devkit 本文根据官方推荐使用Gradle。 1. 插件开发环境 IDEA: 社区版本 Project JDK: 1.8 Gradle: 4.10","text":"Intellij IDEA插件开发有两种方式： Gradle Plugin Devkit 本文根据官方推荐使用Gradle。 1. 插件开发环境 IDEA: 社区版本 Project JDK: 1.8 Gradle: 4.10 2. 确认Gradle可用菜单Preferences -&gt; Plugins 3. 创建Plugin项目 （官方推荐勾选“Use default cradle wrapper”，以便IDEA自动安装Gradle需要的包） 项目创建完成。 工程结构： plugin.xml文件内容： id：当前插件的唯一id号。 name：插件的名称。 version：插件的版本号。 vendor：开发人的邮箱、公司名称。 description：插件的描述，如果将插件上传到IDEA的仓库，在进行下载时会显示该描述。 idea-version：表示当前插件所支持的所有IDEA版本。 extensions：一般放一些我们自己扩展的东西，比如新增高亮显示、新增语言支持。 actions：新增的类在这里注册，用于菜单栏扩展。 4. 配置Gradle插件在build.gradle文件中，设置运行插件的沙箱地址。 5. 创建一个action 自定义功能加在Window菜单栏下。 在plugin.xml文件中，项目自动生成action配置： 6. Gradle运行配置菜单Edit Configurations -&gt; Run/Debug Configurations 点击’+’号，新建Gradle Run Configuration。 7. 运行项目 在Window菜单栏加入我们自定义的’Greeting’选项，点击弹出’Hello World!’。 8. 打包插件参考文献： IDEA官方插件开发手册http://www.jetbrains.org/intellij/sdk/docs/basics.html","categories":[{"name":"Notes","slug":"Notes","permalink":"https://lullabychen.github.io/categories/Notes/"}],"tags":[{"name":"Intellij Plugin","slug":"Intellij-Plugin","permalink":"https://lullabychen.github.io/tags/Intellij-Plugin/"}]},{"title":"Hello World","slug":"hello-world","date":"2019-01-24T11:03:39.000Z","updated":"2019-01-24T11:03:39.000Z","comments":true,"path":"2019/01/24/hello-world/","link":"","permalink":"https://lullabychen.github.io/2019/01/24/hello-world/","excerpt":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub.","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"Keras学习（一）：Mac OS下安装基于TensorFlow的Keras","slug":"Keras1","date":"2019-01-24T01:20:32.000Z","updated":"2019-01-25T04:23:35.000Z","comments":true,"path":"2019/01/24/Keras1/","link":"","permalink":"https://lullabychen.github.io/2019/01/24/Keras1/","excerpt":"操作系统：macOs High Sierra 10.13.6 1. TensorFlow安装必备：Python 采用pip方式安装TensorFlow，命令如下： 1sudo pip install tensorflow 在输入该命令过程中遇到问题如下：（如无遇到可跳过） 1Could not find a version that satisfies the requirement tensorflow ...... 原因是python版本问题：最新的anaconda中python版本已经更新到python3.7，而tensorflow只支持到python3.6。","text":"操作系统：macOs High Sierra 10.13.6 1. TensorFlow安装必备：Python 采用pip方式安装TensorFlow，命令如下： 1sudo pip install tensorflow 在输入该命令过程中遇到问题如下：（如无遇到可跳过） 1Could not find a version that satisfies the requirement tensorflow ...... 原因是python版本问题：最新的anaconda中python版本已经更新到python3.7，而tensorflow只支持到python3.6。 在anaconda官网中给出了三种解决方案： 选择第二种方案，在命令行输入如下命令： 1sudo conda install python=3.6 python3.6安装完成。再如上输入命令，tensorflow安装完成。 2. Keras安装采用pip方式安装Keras，命令如下： 1sudo pip install keras Keras安装完成。 3. 实例测试","categories":[{"name":"Notes","slug":"Notes","permalink":"https://lullabychen.github.io/categories/Notes/"}],"tags":[{"name":"Keras","slug":"Keras","permalink":"https://lullabychen.github.io/tags/Keras/"}]}]}